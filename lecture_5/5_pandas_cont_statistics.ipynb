{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN BELOW IF IN COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/bharris12/URP_2021_Programming_Course/raw/main/lecture_5/data.zip\n",
    "!unzip data.zip\n",
    "!rm data.zip\n",
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T02:18:28.230158Z",
     "start_time": "2019-06-11T02:18:28.222075Z"
    }
   },
   "outputs": [],
   "source": [
    "#Two Plotting Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#Mathematical libraries\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Reading in data and principles of Tidy Data\n",
    "2. Processing data to prepare for plotting\n",
    "3. Principles of plotting\n",
    "    1. Colors\n",
    "    2. Showing distributions faithfully\n",
    "    3. Scaling for the medium of communication\n",
    "4. Matplotlib/Seaborn Grammar\n",
    "5. Some plotting exercises\n",
    "\n",
    "\n",
    "Some of this might be review, but in bioinformatics/data analysis data processing and plotting/communicating are the first and last steps of coding a data-anlysis pipeline. \n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-e7fd43c8c36487389f7bf4f19c52ac2d\" />\n",
    "\n",
    "You will be doing plotting in every step of the data analysis pipeline, so getting comfortable with plotting is crucial\n",
    "\n",
    "Justin's lecture's about statistics have already covered some of the explore and model part of an analysis, and most of the rest of the lectures will be focusing on a few different examples of those. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background/Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Introduction to the biology in this lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the data in this lecture comes from single cell RNA sequencing (scRNAseq). scRNAseq is a relatively new, but rapidly growing technology for assaying 100s or 1000s and even occasionally millions of individual cells' transcriptomes. \n",
    "\n",
    "\n",
    "In short, cells can be separated and barcoded using random DNA sequences before sequencing like normal RNAseq in a variety of different ways. Each method has it's tradeoff. But one constant trait of all the data is sparsity. There are lots of 0s in the data because you are getting a very incomplete sampling of the transcriptome. Yet the method can be extremely useful in studying cell type diversity.\n",
    "\n",
    "<img src=\"https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fnprot.2017.149/MediaObjects/41596_2018_Article_BFnprot2017149_Fig1_HTML.jpg?as=webp\" width='500px'/>\n",
    "(Svensson et al 2018)\n",
    "\n",
    "\n",
    "The data I am including is all from the mouse motor cortex\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQcdINthsxaWhyKIu3qG07_G-ltSLmptR4ZFoGqs1GEVdz1SDbs\" />\n",
    "\n",
    "The data was generated to specifically look at neuron diversity. There are two main types for neurons, **excitatory**, also known as glutamatergic , and **inhibitory**, also known as GABAergic. The excitatory cell types are named by the layers in the brain they are located in, while the inhibitory ones are named by gene markers.\n",
    "\n",
    "<img src=\"http://retina.umh.es/webvision/imageswv/BasicCells.jpg\" />\n",
    "\n",
    "A major focus in studying these cell types is learning how they develop and differentiate from each other. In the case of excitatory cells they develop in the column they will exist in and move from the deeper to shallower layers. For inhibitory cells they develop in another part of the brain, known as the Ganglionic Eminence, and migrate to the cortex. The first major split in defining interneurons is whether they originated in the Medial Ganglionic Eminence (MGE) or the Caudal Ganglionic Eminence (CGE).\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/321972539/figure/fig1/AS:573998834880512@1513863394719/Migratory-streams-originating-from-the-caudal-ganglionic-eminence-CGE-during-mouse.png\" />\n",
    "\n",
    "\n",
    "#### Some terms I use in the data \n",
    "\n",
    "Centroid : The average expression of a cell type. Instead of representing a cell type as all of the cells individually I represent cells as centroids sometimes, where I merely take the average expression of each gene for every cell type\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and processing Columnar/Rectangular Data\n",
    "\n",
    "#### Flat Files\n",
    "\n",
    "Flat files can be opened and viewed in any text editor or excel or in the command line. Larger files will give Excel, text editors and your computer issues, so sometimes command line tools are necessary. \n",
    "\n",
    "Can be directly viewed in the command line:\n",
    "\n",
    "Print out entire file:\n",
    ">$ cat file1.txt \n",
    "\n",
    "Print out the top 10 lines:\n",
    ">$ head file1.txt\n",
    "\n",
    "Print out the top x lines:\n",
    ">$ head -n x file1.txt\n",
    "\n",
    "Print out but allow for scrolling back through output (To escape press q)\n",
    ">$ less file1.txt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The most common file formats are .txt (text file), .tsv (tab separated value), and .csv (comma separated value)\n",
    "\n",
    "Columnar data always has a something encoded in the file to separate each column. Depending on the tool you are using to read in the data it can be called different things but the most common names are:\n",
    "\n",
    "* delimiter (delim)\n",
    "* separator (sep)\n",
    "* IFS \n",
    "\n",
    "The most common separators are:\n",
    "\n",
    "* '\\t' (tab)\n",
    "* ',' (comma)\n",
    "* ' ' (space)\n",
    "* '\\n' (newline)\n",
    "\n",
    "\n",
    "If you are unsure of what the separator is you can use a head to print out the top of the file \n",
    "\n",
    "When reading in these files to python the most common/best functions we use are from numpy and pandas\n",
    "\n",
    "\n",
    "* np.genfromtxt()\n",
    "* pd.read_csv() \n",
    "\n",
    "> If you google ways to read in text files into python you will find many more ways to read in flat files, but these two functions are the most automated.\n",
    "\n",
    "I will discuss a few examples of when to use which function after I give a little more info about files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T21:31:18.459145Z",
     "start_time": "2019-05-13T21:31:18.455178Z"
    }
   },
   "source": [
    "#### Non-flat/binary\n",
    "1. .xlxs (Excel File Format) pd.read_excel\n",
    "2. .h5/.h5f5 (Hierarchical Data Format) pd.read_hdf() or h5py\n",
    "    1. Often used for big data\n",
    "    2. Integrated file structure for storing multiple tables of data together (both expression and metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples\n",
    "**Your computer doesn't actually care about file extensions, so it isn't uncommon to see a file with a different extension that is really just a flat file** \n",
    "\n",
    "All the files below have 100 rows of 10 random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T21:51:27.647766Z",
     "start_time": "2019-05-13T21:51:27.528793Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head -n 1 ./data/file1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T21:51:39.156094Z",
     "start_time": "2019-05-13T21:51:39.036742Z"
    }
   },
   "outputs": [],
   "source": [
    "!head -n 1 ./data/file2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T22:04:46.672766Z",
     "start_time": "2019-05-13T22:04:46.551628Z"
    }
   },
   "outputs": [],
   "source": [
    "!head -n 1 ./data/file2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output the filename is .csv but the file is tab separated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The computer reads in your data 1 row at a time, and will interpret a lot about the data based on the first row**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T21:52:09.050233Z",
     "start_time": "2019-05-13T21:52:08.932733Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head -n 1 ./data/file3.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With rectangular data that we are generally working with sometimes there is missing data. As you can see in this modified version of file3.csv there are two commas in a row and only 9 numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T22:01:21.674374Z",
     "start_time": "2019-05-13T22:01:21.554329Z"
    }
   },
   "outputs": [],
   "source": [
    "!head -n 2 ./data/file3_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T22:01:12.638961Z",
     "start_time": "2019-05-13T22:01:12.633423Z"
    }
   },
   "outputs": [],
   "source": [
    "np.genfromtxt('./data/file3_1.csv',delimiter=',')[:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that numpy has replaced that value with a nan. And still interprets the file as having 10 elements in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T22:09:52.582133Z",
     "start_time": "2019-05-13T22:09:52.567667Z"
    }
   },
   "outputs": [],
   "source": [
    "np.genfromtxt('./data/file3_2.csv',delimiter=',')[:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I remove the second comma numpy will throw an error when I read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T12:27:29.601661Z",
     "start_time": "2019-05-14T12:27:29.594592Z"
    }
   },
   "outputs": [],
   "source": [
    "np.genfromtxt('./data/file3_2.csv',delimiter=',', skip_header=1)[:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if I skip the first row it the file will be read in, just will be 1 row shorter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the examples I have shown so far has been instances where all of the data is numbers, but we often work with both numerical and categorical data. Numpy does not allow for multiple data types in a single array. \n",
    "\n",
    "Numpy defaults to reading in data as a numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:39:13.723057Z",
     "start_time": "2019-06-12T18:39:13.705479Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.genfromtxt('./data/palmer_species.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you tell numpy that you have string data (str) then it will make the entire array strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:39:25.201506Z",
     "start_time": "2019-06-12T18:39:25.193981Z"
    }
   },
   "outputs": [],
   "source": [
    "np.genfromtxt('./data/palmer_species.csv',delimiter=',',dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using numpy to read in the data you can use pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:13:25.414718Z",
     "start_time": "2019-05-14T23:13:25.401733Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv('./data/palmer_data.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas creates, what are called dataframes when you read in data. They have 3 main components. An index (the rownames), columns (column names) and values (data). \n",
    "\n",
    "The default nature of read_csv() is to interpret the first row of the data as the header, and to treat every column as a part of the data. It just sets the index to [0,nrows).\n",
    "\n",
    "In the case of the data I read in above, you can see that the first row of output is bolded, but looks like data, not column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:13:20.511778Z",
     "start_time": "2019-05-14T23:13:20.496929Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv('./data/palmer_data.csv', header=None).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather telling pandas that there is no header will read in the data correctly. Now you if you look at the first column, the one labeled 0, it has the same values as the index column. That is because this column is supposed to be the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:16:29.808101Z",
     "start_time": "2019-05-14T23:16:29.793449Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv('./data/palmer_data.csv', header=None, index_col=0).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see that the first column has been moved to the index location in the output.\n",
    "\n",
    "At this point you might notice and inconsistency between the way pandas is treating the index (rownames) and columns (column names). There is a specific reason for this. \n",
    "\n",
    "Pandas subscribes to a data format/philosophy known as tidy data. \n",
    "\n",
    "Jeff Leek in his book The Elements of Data Analytic Style summarizes the characteristics of tidy data as the points\n",
    "\n",
    "1. Each variable you measure should be in one column.\n",
    "2. Each different observation of that variable should be in a different row.\n",
    "3. There should be one table for each \"kind\" of variable.\n",
    "4. If you have multiple tables, they should include a column in the table that allows them to be linked.\n",
    "\n",
    "    (Tidy Data Wikipedia Article)[https://en.wikipedia.org/wiki/Tidy_data]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about the penguin data that we are looking you can read about it [here](https://allisonhorst.github.io/palmerpenguins/articles/intro.html)\n",
    "\n",
    "<img src='https://allisonhorst.github.io/palmerpenguins/man/figures/lter_penguins.png' width='500px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review Questions and Mini Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. When would you use `np.genfromtxt()` vs `pd.read_csv()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What are the core principles of tidy data and why does expression data not work well for it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T18:21:54.524501Z",
     "start_time": "2019-05-21T18:21:54.520851Z"
    }
   },
   "source": [
    "3. Read in diogo_data.csv properly and store as variable named as diogo1_df\n",
    "    1. None of the data should have Nan's and if it should be read in as a dataframe it should have the correct rownames and column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-01T15:18:53.268566Z",
     "start_time": "2019-06-01T15:18:53.231949Z"
    }
   },
   "outputs": [],
   "source": [
    "##TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T14:32:06.683816Z",
     "start_time": "2019-06-10T14:32:06.676021Z"
    }
   },
   "source": [
    "4. Read in centroids_1.csv properly and store in variable names centroids_1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T03:35:49.142137Z",
     "start_time": "2019-06-11T03:35:49.125867Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q palmerpenguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "df = load_penguins()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T03:34:38.160324Z",
     "start_time": "2019-05-16T03:34:38.157687Z"
    }
   },
   "source": [
    "#### Groupings need to be listed as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T03:34:41.555184Z",
     "start_time": "2019-05-16T03:34:41.543826Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You last column is species, a categorical variable. This means that we can use it to separate the data out by species "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T14:37:35.426665Z",
     "start_time": "2019-05-16T14:37:35.260534Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.barplot(data=df, x='species', y='bill_length_mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pd.melt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T14:38:10.444246Z",
     "start_time": "2019-05-16T14:38:10.417885Z"
    }
   },
   "source": [
    "But what if I wanted to compare the distributions of different numerical variables. Say see how bill_length and bill_width compare (not within an observation)? \n",
    "\n",
    "To do this you need to make the data \"tall\" using the function pd.melt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T14:41:43.426526Z",
     "start_time": "2019-05-16T14:41:43.415846Z"
    }
   },
   "outputs": [],
   "source": [
    "bills_tall = pd.melt(df[['bill_length_mm', 'bill_depth_mm']])\n",
    "bills_tall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T14:42:01.616947Z",
     "start_time": "2019-05-16T14:42:01.471734Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.barplot(data=bills_tall,x='variable',y='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is great, but you may notice that we have lost the information about which species each observation came from, when making the data tall, you can add another argument to melt that will bring with each value the species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T14:47:39.033798Z",
     "start_time": "2019-05-16T14:47:39.020984Z"
    }
   },
   "outputs": [],
   "source": [
    "bills_tall_species = pd.melt(\n",
    "    df[['bill_length_mm', 'bill_depth_mm', 'species']], id_vars='species')\n",
    "bills_tall_species.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T14:49:22.412916Z",
     "start_time": "2019-05-16T14:49:22.176453Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.barplot(data=bills_tall_species, x='variable', y='value', hue='species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pd.concat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is extremely common for data to come in separate files. But we need to join the files together. To use pd.concat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T03:26:26.360795Z",
     "start_time": "2019-05-17T03:26:26.349511Z"
    }
   },
   "outputs": [],
   "source": [
    "centroids_numerical = pd.read_csv(\n",
    "    './data/centroids_numerical.csv', index_col=0)\n",
    "centroids_numerical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T03:26:26.829683Z",
     "start_time": "2019-05-17T03:26:26.820101Z"
    }
   },
   "outputs": [],
   "source": [
    "centroids_metadata = pd.read_csv('./data/centroids_metadata.csv', index_col=0)\n",
    "centroids_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T18:20:50.751131Z",
     "start_time": "2019-05-21T18:20:50.740830Z"
    }
   },
   "outputs": [],
   "source": [
    "centroids = pd.concat([centroids_numerical, centroids_metadata],axis=1)\n",
    "centroids.head()\n",
    "#The axis=1 tells the function to stick the columns next to eachother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T18:19:33.949396Z",
     "start_time": "2019-05-21T18:19:33.942382Z"
    }
   },
   "outputs": [],
   "source": [
    "split_1 = centroids_metadata.iloc[:50]\n",
    "split_2 = centroids_metadata.iloc[50:]\n",
    "centroids_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T18:19:34.941115Z",
     "start_time": "2019-05-21T18:19:34.934200Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([split_2,split_1]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time without the axis=1 it sticks the two dataframes on top of each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computations on existing columns to create new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T18:20:53.728018Z",
     "start_time": "2019-05-21T18:20:53.718509Z"
    }
   },
   "outputs": [],
   "source": [
    "centroids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T18:20:54.025281Z",
     "start_time": "2019-05-21T18:20:54.021081Z"
    }
   },
   "outputs": [],
   "source": [
    "#Sometimes you need to add columns based on other columns\n",
    "centroids['Ogt_zscore'] = stats.zscore(centroids['Ogt'])\n",
    "centroids['Cacna1a_zscore'] = stats.zscore(centroids['Cacna1a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T18:20:54.456434Z",
     "start_time": "2019-05-21T18:20:54.445784Z"
    }
   },
   "outputs": [],
   "source": [
    "centroids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review questions and mini exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the difference between using and not using axis=1 with pd.conca()?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load in example_expression.csv and example_metadata.csv. Then concat them together into a single dataframe named example_df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T21:21:47.030697Z",
     "start_time": "2019-06-12T21:21:47.028379Z"
    }
   },
   "outputs": [],
   "source": [
    "##TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T18:44:20.429668Z",
     "start_time": "2019-05-13T18:44:20.427303Z"
    }
   },
   "source": [
    "## Exercise \n",
    "\n",
    "Goal of analysis: Calculate differential expression between MGE and CGE \n",
    "\n",
    "This test is similar to the T-test, however it is non-parametric. Instead of using the Gaussian or normal distribution as a null we make the data uniformly distributed and compare the ranks of the samples for each gene. This makes it more robust to outliers in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and preprocess the data\n",
    "\n",
    "1. Read in data\n",
    "2. Make some descriptive plots about the metadata (sample sizes and stuff)\n",
    "    1. Whatever you think is necessary to understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T21:50:09.014382Z",
     "start_time": "2019-06-10T21:50:08.613169Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T21:50:25.391682Z",
     "start_time": "2019-06-10T21:50:25.380523Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T21:52:58.121551Z",
     "start_time": "2019-06-10T21:52:58.004583Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO (Make an informative plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T19:57:50.957750Z",
     "start_time": "2019-05-13T19:57:50.953625Z"
    }
   },
   "source": [
    "### Calculating Mann Whitney U test statistic:\n",
    "$U_1 = R_1 - ((n_1 * (n_1 + 1)) /2)$\n",
    "\n",
    "$U_2 = R_2 - ((n_2 * (n_2 + 1)) /2)$\n",
    "\n",
    "$U = min(U_1, U_2)$\n",
    "\n",
    "$n_1$ = sample size of group 1\n",
    "\n",
    "$n_2$ = sample size of group 2\n",
    "\n",
    "R is sum of the rank (order) of the values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T21:22:53.069161Z",
     "start_time": "2019-06-12T21:22:53.066469Z"
    }
   },
   "outputs": [],
   "source": [
    "##TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T19:54:59.339728Z",
     "start_time": "2019-05-13T19:54:59.336337Z"
    }
   },
   "source": [
    "### Calculating AUROC from MannWhitney U:\n",
    "\n",
    "<img src=\"https://glassboxmedicine.files.wordpress.com/2019/02/roc-curve-v2.png?w=576\" />\n",
    "\n",
    "The AUROC is a metric used to asses the quality of how well we can predict something. For this case we are measuring how well a gene predicts a specific cell type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large{AUROC = 1 - \\frac{U}{n_1 * n_2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint: AUROCs are values between 0 and 1, but mostly should be between .5 and 1, use a plot to check your results*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T01:59:27.427709Z",
     "start_time": "2019-06-11T01:59:27.424187Z"
    }
   },
   "outputs": [],
   "source": [
    "##TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T01:29:18.848541Z",
     "start_time": "2019-06-11T01:29:18.703974Z"
    }
   },
   "outputs": [],
   "source": [
    "##TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Log2FC\n",
    "\n",
    "You calculate log2FC by subtracting the average expressions from each other for each gene. Then taking the log2 of the difference. \n",
    "\n",
    "Notes:\n",
    "* You need to store the sign because log doesn't take negative numbers\n",
    "* You need to add a psuedocount (1) because log doesn't take 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T01:58:38.014691Z",
     "start_time": "2019-06-11T01:58:38.000420Z"
    }
   },
   "outputs": [],
   "source": [
    "##TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Volcano Plots\n",
    "\n",
    "Volcano plots are a common way of displaying results from differential expression. On the X axis you plot the Log2FC and on the Y axis you can plot the AUROC or log10(P value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T01:59:45.986539Z",
     "start_time": "2019-06-11T01:59:45.836142Z"
    }
   },
   "outputs": [],
   "source": [
    "##TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating P values (Optional):\n",
    "\n",
    "$\\large{Z = \\frac{|U - \\frac{n_1 * n_2}{2}|}{\\sqrt{\\frac{n_1 * n_2 * (n_1 + n_2 + 1)}{12}}}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T01:31:24.630253Z",
     "start_time": "2019-06-11T01:31:24.625434Z"
    }
   },
   "outputs": [],
   "source": [
    "##TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the U statistic is making the values normally distributed so you can then convert those to p values using a normal distribution. \n",
    "\n",
    "<img src=\"http://www.z-table.com/uploads/2/1/7/9/21795380/7807141_orig.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T02:01:01.363540Z",
     "start_time": "2019-06-11T02:01:01.215487Z"
    }
   },
   "outputs": [],
   "source": [
    "##TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we could have predicted this result?\n",
    "\n",
    "A post-doc in the Gillis lab (Maggie Crow) analyzed hundreds of results of differential expression experiments and showed that certain genes are more often differentially expressed than others. \n",
    "\n",
    "<img src=\"https://www.pnas.org/content/pnas/116/13/6491/F2.medium.gif\" />\n",
    "\n",
    "As we learned before, AUROCs show how good a prediction is, so the list of genes Maggie came up with has an AUROC of .83, which is exceptionally good. \n",
    "\n",
    "This doesn't mean that just because the results are predictable that they aren't interesting, just that it isn't surprising that these results would look like results from other studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T02:02:25.226088Z",
     "start_time": "2019-06-11T02:02:25.222856Z"
    }
   },
   "outputs": [],
   "source": [
    "corrected = sm.stats.multipletests(p,method='fdr_bh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T02:03:04.655168Z",
     "start_time": "2019-06-11T02:03:04.650009Z"
    }
   },
   "outputs": [],
   "source": [
    "results['p_raw'] = p\n",
    "results['is_sig'] = corrected[0]\n",
    "results['p_adj'] = corrected[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T02:05:33.539908Z",
     "start_time": "2019-06-11T02:05:33.513578Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "de_prior = pd.read_csv('./data/mouse_de_prior.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T02:05:50.153832Z",
     "start_time": "2019-06-11T02:05:50.143437Z"
    }
   },
   "outputs": [],
   "source": [
    "de_prior.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T02:09:44.449181Z",
     "start_time": "2019-06-11T02:09:44.245601Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(\n",
    "    de_prior.loc[results[results.is_sig].index, 'MF.rank'].dropna(),\n",
    "    kde=False,\n",
    "    norm_hist=True)\n",
    "sns.distplot(de_prior['MF.rank'], ax=ax, kde=False, norm_hist=True)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL and databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large tabular data can often use special software for accessing and manipulating data. A common framework for this is known as SQL. Tools like [biomart](http://useast.ensembl.org/biomart/martview/3d270413fa0c7c3dca475d573cbf4897) can be accessed through a web interface or using SQL. SQL can be useful if you need to make repeated or many queries of a bioinformatics database (as an aside R has a package biomarRT for querrying biomart). Using SQL will feel a little similar to pandas.\n",
    "\n",
    "[SQL Wiki](https://en.wikipedia.org/wiki/SQL)\n",
    "\n",
    "[Biomart Public SQL server info](https://useast.ensembl.org/info/data/mysql.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some weird \"common file formats\"\n",
    "\n",
    "Some bioinformatics software require specific file formats that are subclasses of other file formats\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "[GCT: A Special Flat Files for Gene Expression](http://software.broadinstitute.org/cancer/software/gsea/wiki/index.php/Data_formats)\n",
    "\n",
    "[Loom: A Special HDf5 Files for Single Cell Data](http://linnarssonlab.org/loompy/index.html)\n",
    "\n",
    "Both of these filetypes utilize flat/hdf5 file formats but format them in specific ways to make it easy for the software that they were created for to parse them"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "286.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
