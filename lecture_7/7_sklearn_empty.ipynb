{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install palmerpenguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics,datasets,neighbors\n",
    "from palmerpenguins import load_penguins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sample data and poke around to understand it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we'll use a classic sample dataset called the Palmer Penguins. There will be more details below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguinsdf = load_penguins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguinsdf.head()\n",
    "penguinsdf.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguinsdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another dataset that we are gonna work on is the iris dataset again. But this time we will be loading it directly from sklearn.datasets. And they way its stored is slightly different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "type(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's an sklearn bunch?? Looking at the documentation for the sklearn iris dataset (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html), we see that a bunch is just a python dictionary. Let's try interacting with it as we would a dictionary and seeing the what the keys are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We see that there are 6 keys. Let's take a look at the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index the iris dictionary to see the description\n",
    "print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a nice description and background of the classic iris dataset. Now, let's poke around and see what the other things in the iris dictionary are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print each of the remaining values in the iris dictionary by indexing the dictionary with the keys\n",
    "#dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n",
    "print(iris['filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, it's clear these values of the iris dataset appear as we expected from the documentation. Let's store the data in a pandas dataframe to more easily manipulate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a pandas dataframe to store the data and a pandas series to store the targets or y labels\n",
    "irisdf = pd.DataFrame(data=iris['data'], index=range(1,151,1), columns=iris['feature_names'])\n",
    "ylabels = pd.Series(data=iris['target'], index=range(1,151,1), name=\"iris type\")\n",
    "irisdf['label'] = ylabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's check that our dataframe looks as we expect\n",
    "irisdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's check the series\n",
    "ylabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some visualizations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairplot_figure = sns.pairplot(irisdf, hue='label',palette='colorblind')\n",
    "pairplot_figure.fig.set_size_inches(9, 6.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which correlation looks most linear? Let's plot the petal length against petal width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = [\"petal length (cm)\"]\n",
    "target_column = \"petal width (cm)\"\n",
    "_ = sns.scatterplot(data=irisdf, x=data_columns[0], y=target_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try building a linear regression model that use petal length to predict petal width. First we need to split the data in to train/test sets using a 50/50 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitdata(data, testratio):\n",
    "    #set seed so train and test will always split the same\n",
    "    np.random.seed(42)\n",
    "    shuffindices = np.random.permutation(len(data))\n",
    "    testsize = int(len(data) * testratio)\n",
    "    testindices = shuffindices[:testsize]\n",
    "    trainindices = shuffindices[testsize:]\n",
    "    return data.iloc[trainindices], data.iloc[testindices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the test/train set\n",
    "iristrain, iristest = splitdata(irisdf, 0.5)\n",
    "y_train = iristrain['petal width (cm)']\n",
    "y_test = iristest['petal width (cm)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see dimensions fit each other\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the linear model\n",
    "reg = LinearRegression().fit(iristrain.loc[:,[\"petal length (cm)\"]], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R^2 is one of the most commonly used metrics to evaluate a linear regression fit: https://en.wikipedia.org/wiki/Coefficient_of_determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how good is the fit based on the R^2 coefficient of determination\n",
    "reg.score(iristrain.loc[:,[\"petal length (cm)\"]], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the held out test data using the linear regression model trained on the train set of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = reg.predict(iristest.loc[:,[\"petal length (cm)\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets visualize how well the linear regression did in predicting the held out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax =sns.scatterplot(x = predictions, y = y_test)\n",
    "ax.set(xlabel='prediction', ylabel='target')\n",
    "r2 = reg.score(iristest.loc[:,[\"petal length (cm)\"]], y_test)\n",
    "ax.text(.05, .8, 'r={:.2f}'.format(r2),\n",
    "            transform=ax.transAxes)\n",
    "x0, x1 = ax.get_xlim()\n",
    "y0, y1 = ax.get_ylim()\n",
    "lims = [max(x0, y0), min(x1, y1)]\n",
    "ax.plot(lims, lims, '-r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try the same process we just done on the palmer penguin dataset. Can you identify features that are linearly correlated? Can one feature help predicting the target feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot visulization of the palmer penguins dataset, identify linearly related features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot of the features you identified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the test/train set, you can directly use the defined splitdata function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the linear model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how good is the fit based on the R^2 coefficient of determination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the held out test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute r2 and plot prediction against ground truth of test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA, or principla component analysis is a commonly used dimension reduction algorithm : https://en.wikipedia.org/wiki/Principal_component_analysis\n",
    "Principal components can be considered as linear combinations of features. \n",
    "It projects each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take numerical columns out of df\n",
    "numerics = ['float16', 'float32', 'float64']\n",
    "numdf = irisdf.select_dtypes(include=numerics)\n",
    "numdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or, for the iris dataset thats stored as sklearn bunch\n",
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit PCA model\n",
    "pca =PCA(n_components=4)\n",
    "pca.fit(numdf)\n",
    "X = pca.transform(numdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct PCA output into a dataframe\n",
    "pca_df = pd.DataFrame(data=X,columns = ['PC1','PC2','PC3','PC4'])\n",
    "pca_df['label'] = iris.target\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize PCA with label information\n",
    "sns.scatterplot(x =\"PC1\",y=\"PC2\",data=pca_df,hue='label',palette='colorblind')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to replicate the PCA analysis on the palmer penguin datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take numerical columns out of df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct PCA output into a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also Try plotting with different dimensions, see how it influences the way data is presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
